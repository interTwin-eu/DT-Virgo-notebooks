{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e9a661-b1ba-49f3-b8a1-8f495897ff54",
   "metadata": {},
   "source": [
    "# Create Centered Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481936b-9605-46b9-b012-f8496bdb78e0",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab63c3-be87-4662-9fe3-36e200224f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "already_extracted = False\n",
    "\n",
    "if not already_extracted:\n",
    "\n",
    "    with tarfile.open('./data/O3a_SL_aux.tar.gz') as tar:\n",
    "        tar.extractall(path='./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569bad50-5fe3-42bd-8013-d257fe903b47",
   "metadata": {},
   "source": [
    "## Create the list of available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5c2a8-9aa6-42fd-b9fe-7cfd779e1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('./data/O3a_scatt_light_aux/'):\n",
    "    os.makedirs('./data/O3a_scatt_light_aux/')\n",
    "\n",
    "directory = './data/O3a_scatt_light_aux/'\n",
    "\n",
    "files_paths = [elem for elem in [os.path.join(directory, path) for path in os.listdir(directory)] if elem[-3:] == '.h5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfbe8b-a6cd-4c75-a574-01b91904f785",
   "metadata": {},
   "source": [
    "## Plot the q-scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54b60a-23b9-4e69-89e5-00c013e0713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5 \n",
    "from gwpy.timeseries import TimeSeries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "### Gather file\n",
    "file_dir = files_paths[0]\n",
    "fid = h5.File(file_dir, 'r')\n",
    "\n",
    "### Gather data of all channels from file\n",
    "grp_name = file_dir[:-3].split('/')[-1]\n",
    "group_data = fid[grp_name]\n",
    "\n",
    "### Gather data of strain channel\n",
    "strain_channel = group_data['V1:Hrec_hoft_16384Hz']\n",
    "\n",
    "### Create TimeSeries object\n",
    "t = TimeSeries(strain_channel[()])\n",
    "t.t0 = strain_channel.attrs['t0']\n",
    "t.dt = 1.0 / strain_channel.attrs['sample_rate']\n",
    "\n",
    "### Create q-scan in the 2 seconds interval\n",
    "t0_scan = (t.times[0] + (t.times[-1] - t.times[0]) / 2).value\n",
    "dt_scan = 0.5\n",
    "hq = t.q_transform(outseg=(t0_scan - dt_scan, t0_scan + dt_scan), frange=(10, 100))\n",
    "\n",
    "### Create figure and mesh with this period scan\n",
    "fig = plt.figure(frameon=False, figsize=(3.21, 2.56), dpi=100)  # figsize values are set accordingly to obtain a 256x256 image\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "mesh = ax.pcolormesh(hq)\n",
    "plt.close(fig)\n",
    "\n",
    "### Get maximum energy value from the mesh\n",
    "mesh_data = mesh.get_array()\n",
    "mesh_max = np.amax(mesh_data)\n",
    "indices = np.where(mesh_data == mesh_max)\n",
    "index = indices[0][0] % hq.times.value.shape[0]\n",
    "time_val = hq.times.value[index]\n",
    "dt_max = 0.25 # Interval of time around maximum value\n",
    "\n",
    "### Create q-scan in the proper time period\n",
    "hq = t.q_transform(outseg=(time_val - dt_max, time_val + dt_max), frange=(10, 100))\n",
    "\n",
    "### Create figure with proper size and no frame\n",
    "fig = plt.figure(frameon=False, figsize=(3.21, 2.56), dpi=100)  # figsize values are set accordingly to obtain a 256x256 image\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "mesh = ax.pcolormesh(hq)\n",
    "mesh.set_clim(0, 26)\n",
    "cbar = fig.colorbar(mesh, label=\"Normalised energy\")\n",
    "ax.grid(False)\n",
    "ax.axis('off')\n",
    "ax.set_yscale('log')\n",
    "cbar.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf5f548-e946-452a-900c-4d8b7054aa5c",
   "metadata": {},
   "source": [
    "## Plot q-scan for all the auxiliary channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a850fa-5599-43d8-89f4-f6ace3a49ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = []\n",
    "    \n",
    "for key in group_data.keys():\n",
    "    \n",
    "    dataset = group_data[key]\n",
    "    \n",
    "    # Create TimeSeries\n",
    "    t = TimeSeries(dataset[()])\n",
    "    t.t0 = dataset.attrs['t0']\n",
    "    t.dt = 1.0 / dataset.attrs['sample_rate']\n",
    "    \n",
    "    # Calculate q-transform\n",
    "    t0_scan = t.times[t.times.shape[0]//2].value\n",
    "    dt_scan = 2.0\n",
    "    hq = t.q_transform(outseg=(t0_scan - dt_scan, t0_scan + dt_scan), frange=(10, 100))\n",
    "    \n",
    "    # Create Figure\n",
    "    fig = plt.figure(frameon=False, figsize=(3.21, 2.56), dpi=200)  # figsize values are set accordingly to obtain a 256x256 image\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    mesh = ax.pcolormesh(hq)\n",
    "    mesh.set_clim(0, 30)\n",
    "    cbar = fig.colorbar(mesh, label=\"Normalised energy\")\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    ax.set_yscale('log')\n",
    "    cbar.remove()\n",
    "    plt.close(fig)\n",
    "        \n",
    "    figures.append(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad8f79-c96f-4ced-a5b8-37cd9c83affb",
   "metadata": {},
   "source": [
    "## Compress the images into a single large one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ceb315-f180-4693-85b8-96b5d82f52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "def figures_to_tensors(figs):\n",
    "    tensor_figs = []\n",
    "    transform = transforms.ToTensor()\n",
    "    \n",
    "    for fig in figs:\n",
    "        fig.canvas.draw()\n",
    "        buffer = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "        image = Image.fromarray(buffer[:256, :256, :3])\n",
    "        tensor_figs.append(transform(image))\n",
    "    \n",
    "    return tensor_figs\n",
    "\n",
    "tens = figures_to_tensors(figures)\n",
    "final_fig = torch.cat([tensor for tensor in tens], -1)\n",
    "transform = transforms.ToPILImage()\n",
    "final_image = transform(final_fig)\n",
    "\n",
    "final_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301ab8f-270b-46a3-a902-d3f41c2e3937",
   "metadata": {},
   "source": [
    "## Put all the preciding into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a4520-df94-4216-93b3-68b7c5174e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(file_number):\n",
    "\n",
    "    ### Gather file\n",
    "    file_dir = files_paths[file_number]\n",
    "    fid = h5.File(file_dir, 'r')\n",
    "\n",
    "    ### Gather data of all channels from file\n",
    "    grp_name = file_dir[:-3].split('/')[-1]\n",
    "    group_data = fid[grp_name]\n",
    "    \n",
    "    figures = []\n",
    "    \n",
    "    for key in group_data.keys():\n",
    "\n",
    "        dataset = group_data[key]\n",
    "\n",
    "        # Create TimeSeries\n",
    "        t = TimeSeries(dataset[()])\n",
    "        t.t0 = dataset.attrs['t0']\n",
    "        t.dt = 1.0 / dataset.attrs['sample_rate']\n",
    "\n",
    "        # Calculate q-transform\n",
    "        t0_scan = t.times[t.times.shape[0]//2].value\n",
    "        dt_scan = 0.5\n",
    "        hq = t.q_transform(outseg=(t0_scan - dt_scan, t0_scan + dt_scan), frange=(10, 100))\n",
    "\n",
    "        ### Create figure and mesh with this period scan\n",
    "        fig = plt.figure(frameon=False, figsize=(3.21, 2.56), dpi=100)  # figsize values are set accordingly to obtain a 256x256 image\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        mesh = ax.pcolormesh(hq)\n",
    "        plt.close(fig)\n",
    "\n",
    "        ### Get maximum energy value from the mesh\n",
    "        mesh_data = mesh.get_array()\n",
    "        mesh_max = np.amax(mesh_data)\n",
    "        indices = np.where(mesh_data == mesh_max)\n",
    "        index = indices[0][0] % hq.times.value.shape[0]\n",
    "        time_val = hq.times.value[index]\n",
    "        dt_max = 0.25  # Interval of time around maximum value\n",
    "\n",
    "        ### Create q-scan in the proper time period\n",
    "        hq = t.q_transform(outseg=(time_val - dt_max, time_val + dt_max), frange=(10, 100))\n",
    "\n",
    "        # Create Figure\n",
    "        fig = plt.figure(frameon=False, figsize=(3.21, 2.56), dpi=100)  # figsize values are set accordingly to obtain a 256x256 image\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        mesh = ax.pcolormesh(hq)\n",
    "        mesh.set_clim(0, 30)\n",
    "        cbar = fig.colorbar(mesh, label=\"Normalised energy\")\n",
    "        ax.grid(False)\n",
    "        ax.axis('off')\n",
    "        ax.set_yscale('log')\n",
    "        cbar.remove()\n",
    "        plt.close(fig)\n",
    "\n",
    "        figures.append(fig)\n",
    "        \n",
    "    tens = figures_to_tensors(figures)\n",
    "    final_fig = torch.cat([tensor for tensor in tens], -1)\n",
    "    transform = transforms.ToPILImage()\n",
    "    final_image = transform(final_fig)\n",
    "    \n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e13ede-f9dd-45bf-84df-1249fca5b919",
   "metadata": {},
   "source": [
    "## Save the images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a4c08-371a-411e-b64a-d8bdee033b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./all_channels_images/'):\n",
    "    os.makedirs('./all_channels_images/')\n",
    "\n",
    "savedir = './all_channels_images/'\n",
    "\n",
    "already_done = False\n",
    "\n",
    "if not already_done:\n",
    "    for i in range(len(files_paths)):\n",
    "        final_image = plots(i)\n",
    "        final_image.save(f'{savedir}ch_img{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b93bf-0703-4bd6-b8b2-77c42d4f0ee2",
   "metadata": {},
   "source": [
    "## Merge some images into a single big one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735bd20-2d0e-470d-a410-5cafe6f41f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_list = os.listdir(savedir)\n",
    "images_list = [savedir + x for x in images_list if x[-4:] == '.png']\n",
    "images_list.sort(key=lambda x: int(x.split('g')[-2].split('.')[0]))\n",
    "### How many files we want to include in the same picture?\n",
    "images_list = images_list[:20]\n",
    "\n",
    "imgs = [Image.open(i) for i in images_list]\n",
    "\n",
    "min_img_width = min(i.width for i in imgs)\n",
    "\n",
    "total_height = 0\n",
    "for i, img in enumerate(imgs):\n",
    "    # If the image is larger than the minimum width, resize it\n",
    "    if img.width > min_img_width:\n",
    "        imgs[i] = img.resize((min_img_width, int(img.height / img.width * min_img_width)), Image.ANTIALIAS)\n",
    "    total_height += imgs[i].height\n",
    "\n",
    "# Now that we know the total height of all of the resized images, we know the height of our final image\n",
    "img_merge = Image.new(imgs[0].mode, (min_img_width, total_height))\n",
    "y = 0\n",
    "for img in imgs:\n",
    "    img_merge.paste(img, (0, y))\n",
    "\n",
    "    y += img.height\n",
    "img_merge.save('multiple_files_img.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c8666-6a21-45e7-bac8-ba628ad1b42f",
   "metadata": {},
   "source": [
    "### Build Pix2Pix dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10c1ce-4fb3-46f2-940a-51329e7962f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_images = [savedir + x for x in os.listdir(savedir) if x[-4:] == '.png']\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "to_image = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8dcbb-a8f4-4571-aa8c-641b109b2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2  # The aux channel number that we want to consider: should go from 1 to 7\n",
    "\n",
    "if not os.path.exists('./aux_channel_two/'):\n",
    "    os.makedirs('./aux_channel_two/')\n",
    "    \n",
    "ch_img_dir = './aux_channel_two/'\n",
    "train_size = 600\n",
    "\n",
    "for i in range(len(ch_images)):\n",
    "    tens = to_tensor(Image.open(ch_images[i]))\n",
    "\n",
    "    first_tens = tens[:, :, :256]\n",
    "    second_tens = tens[:, :, 256*num:256*(num+1)]\n",
    "\n",
    "    final_tens = torch.cat((first_tens, second_tens), 2)\n",
    "\n",
    "    image = to_image(final_tens)\n",
    "\n",
    "    if i < train_size:\n",
    "        image.save(ch_img_dir + f'train/img{i}.png')\n",
    "    else:\n",
    "        image.save(ch_img_dir + f'test/img{i-train_size}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45eef87-4882-4f12-b3d7-89bcd389ea98",
   "metadata": {},
   "source": [
    "### Bluid cycleGAN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185d4dd-f01b-42e6-b587-e42f87dd69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2  # The aux channel number that we want to consider\n",
    "\n",
    "if not os.path.exists('./aux_channel_two_cycle/'):\n",
    "    os.makedirs('./aux_channel_two_cycle/')\n",
    "\n",
    "ch_img_dir = './aux_channel_two_cycle/'\n",
    "train_size = 600\n",
    "\n",
    "for i in range(len(ch_images)):\n",
    "    tens = to_tensor(Image.open(ch_images[i]))\n",
    "\n",
    "    first_tens = tens[:, :, :256]\n",
    "    second_tens = tens[:, :, 256*num:256*(num+1)]\n",
    "    \n",
    "    strain_tens = torch.cat((first_tens, second_tens), 2)\n",
    "    aux_tens = torch.cat((second_tens, first_tens), 2)\n",
    "\n",
    "    image1 = to_image(strain_tens)\n",
    "    image2 = to_image(aux_tens)\n",
    "\n",
    "    if i < train_size:\n",
    "        image1.save(ch_img_dir + f'train/strain/img{i}.png')\n",
    "        image2.save(ch_img_dir + f'train/aux/img{i}.png')\n",
    "    else:\n",
    "        image1.save(ch_img_dir + f'test/strain/img{i-train_size}.png')\n",
    "        image2.save(ch_img_dir + f'test/aux/img{i-train_size}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
