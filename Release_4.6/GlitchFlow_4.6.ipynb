{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import h5py as h5\n",
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import gwpy\n",
    "    from gwpy.timeseries import TimeSeries\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet gwpy\n",
    "    import gwpy\n",
    "    from gwpy.timeseries import TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we prepare the dataset for NN training and inference.\n",
    "\n",
    "The section is divided in three parts:\n",
    "- 1) **Load Data**, where we load the dataset form datalake\n",
    "- 2) **Split Data**, where we convert the dataset to torch, and then divide it into train and test set (making also a smaller version of the two)\n",
    "- 3) **Normalise Data & Dataloader**, where we normalize the dataset (for NN convergence reasons) and create dataloader objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#128x128 dataset\n",
    "df1=pd.read_pickle('/home/jovyan/Old Image dataset/Cut_Image_128x128_3000.pkl')\n",
    "df2=pd.read_pickle('/home/jovyan/Old Image dataset/Cut_Image_128x128_3000_6000.pkl')\n",
    "df3=pd.read_pickle('/home/jovyan/Old Image dataset/Cut_Image_128x128_6000_9000.pkl')\n",
    "df4=pd.read_pickle('/home/jovyan/Old Image dataset/Cut_Image_128x128_9000_end.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path='/home/jovyan/Qtransform Dataset/Qtransform_18-50Hz_2s_64x64.pt'\n",
    "#path='/home/jovyan/Qtransform Dataset/QT_Hraw_Hrec_q_12_4_s_84x336.pt'\n",
    "#path='/home/jovyan/Qtransform Dataset/QT_Hraw_Hrec_q_12_4_s_84x336_no_whiten.pt'\n",
    "#path='/home/jovyan/Qtransform Dataset/QT_Hraw_Hrec_q_12_4_s_64x256_no_whiten.pt'\n",
    "#path='/home/jovyan/Qtransform Dataset/QT_Hraw_Hrec_q_12_4_s_64x256_no_whiten_8-500Hz_logf.pt'\n",
    "path='/home/jovyan/Qtransform Dataset/QT_Hraw_Hrec_q_12_4_s_64x256_no_whiten_8-500Hz_logf_9channels.pt'\n",
    "#path='/home/jovyan/Qtransform Dataset/QT_Hraw_Hrec_q_12_4_s_128x512_no_whiten_8-500Hz_logf_9channels.pt'\n",
    "#path='/data/notebooks_intertwin/QT_Hraw_Hrec_q_12_4_s_128x512_no_whiten_8-500Hz_logf.pt'\n",
    "try:\n",
    "    del loaded_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "loaded_tensor = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Parameters\n",
    "v_max = 15\n",
    "t_min = 0\n",
    "t_max = loaded_tensor.shape[-1]\n",
    "f_min = 0\n",
    "f_max = loaded_tensor.shape[-2]\n",
    "\n",
    "# Specify which channel is \"strain\" and which channels to use as aux.\n",
    "strain_channel = 0\n",
    "# For example, if you want to ignore channel 1 and use channels 2 onward as aux:\n",
    "aux_channel_indices_all = list(range(1, loaded_tensor.shape[1]))\n",
    "\n",
    "# Parameter: how many aux channels you want to plot?\n",
    "n_aux_desired = len(aux_channel_indices_all)  # (or set to a lower number, e.g., 2)\n",
    "# Select only the desired number of auxiliary channels:\n",
    "aux_channel_indices = aux_channel_indices_all[:n_aux_desired]\n",
    "\n",
    "# Parameter: number of columns for the subplot grid.\n",
    "# (Total plots = 1 (strain) + number of aux channels.)\n",
    "total_plots = 1 + len(aux_channel_indices)\n",
    "ncols = min(total_plots, 3)  # e.g., up to 3 columns per row; adjust as needed.\n",
    "nrows = math.ceil(total_plots / ncols)\n",
    "\n",
    "for i in range(2):\n",
    "    print('---------------------------')\n",
    "    print(f'IMAGE {i}')\n",
    "    \n",
    "    # Select and flip images (if desired)\n",
    "    qplt_strain = torch.flipud(loaded_tensor[i, strain_channel, f_min:f_max, t_min:t_max])\n",
    "    \n",
    "    # Create a list to hold aux images.\n",
    "    aux_images = []\n",
    "    for idx in aux_channel_indices:\n",
    "        aux_img = torch.flipud(loaded_tensor[i, idx, f_min:f_max, t_min:t_max])\n",
    "        aux_images.append(aux_img)\n",
    "\n",
    "    # Create subplots.\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "    \n",
    "    # Flatten axes array for easier indexing.\n",
    "    if nrows * ncols == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # --- Plot Strain ---\n",
    "    im = axes[0].imshow(qplt_strain, aspect='auto', vmin=0, vmax=v_max)\n",
    "    axes[0].set_title('Strain')\n",
    "    axes[0].set_xlabel('Time')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    fig.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # --- Plot each aux channel ---\n",
    "    for j, aux_img in enumerate(aux_images):\n",
    "        ax = axes[j + 1]\n",
    "        im = ax.imshow(aux_img, aspect='auto', vmin=0, vmax=v_max)\n",
    "        ax.set_title(f'Aux {aux_channel_indices[j]}')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        fig.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Turn off any extra (unused) subplots.\n",
    "    for k in range(total_plots, nrows * ncols):\n",
    "        axes[k].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)  # Choose any integer as the seed\n",
    "data=loaded_tensor\n",
    "num_aux_channels=data.shape[1]-1\n",
    "'''\n",
    "# Specify indices of interest along the second dimension\n",
    "aux_indices = torch.tensor([2, 3, 4, 6, 7, 8, 16, 17, 19, 20])\n",
    "num_aux_channels=aux_indices.shape[0]\n",
    "# Select specific auxiliary channels\n",
    "data= loaded_tensor[:, torch.cat([torch.tensor([0]) ,aux_indices],dim=0), :, :]\n",
    "'''\n",
    "\n",
    "# Set split sizes: 90% for training, 10% for testing\n",
    "train_size = int(0.9 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "\n",
    "# Perform the train-test split with the fixed seed\n",
    "train_data_list, test_data_list = random_split(data, [train_size, test_size])\n",
    "\n",
    "\n",
    "# Convert the Subset objects back to tensors\n",
    "train_data = torch.stack([data[idx] for idx in train_data_list.indices])\n",
    "test_data = torch.stack([data[idx] for idx in test_data_list.indices])\n",
    "\n",
    "\n",
    "# Check the final concatenated shapes\n",
    "print(f'{train_data.shape=}\\n{test_data.shape=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(tensor, num_slices):\n",
    "    B, C, H, W = tensor.shape\n",
    "    W0 = H  # Target width is now H\n",
    "    offset = (W - num_slices * W0) // 2\n",
    "\n",
    "    selected_chunks = tensor[:, :, :, offset:offset + num_slices * W0].view(B, C, H, num_slices, W0)\n",
    "    tensor_permuted = selected_chunks.permute(0, 3, 1, 2, 4)\n",
    "    augmented_tensor = tensor_permuted.contiguous().view(B * num_slices, C, H, W0)\n",
    "    return augmented_tensor\n",
    "\n",
    "\n",
    "# Augment training data (3 slices)\n",
    "train_data_augmented_3 = augment_data(train_data, 3)\n",
    "\n",
    "# Augment training data (2 slices)\n",
    "train_data_augmented_2 = augment_data(train_data, 2)\n",
    "\n",
    "train_data_2d = torch.cat([train_data_augmented_3, train_data_augmented_2], dim=0)\n",
    "\n",
    "# Augment validation data (3 slices)\n",
    "val_data_augmented_3 = augment_data(test_data, 3)\n",
    "\n",
    "# Augment validation data (2 slices)\n",
    "val_data_augmented_2 = augment_data(test_data, 2)\n",
    "\n",
    "test_data_2d = torch.cat([val_data_augmented_3, val_data_augmented_2], dim=0)\n",
    "\n",
    "print(train_data_2d.shape)\n",
    "print(test_data_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loaded_tensor\n",
    "del val_data_augmented_3\n",
    "del val_data_augmented_2\n",
    "del train_data_augmented_2\n",
    "del train_data_augmented_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "v_max = 25\n",
    "t_min = 0\n",
    "t_max = train_data_2d.shape[-1]\n",
    "f_min = 0\n",
    "f_max = train_data_2d.shape[-2]\n",
    "\n",
    "# Frequency settings (same as your first code snippet)\n",
    "f_range = (8, 500)\n",
    "t_range=(0,1)\n",
    "desired_ticks = [8, 20, 30, 50, 100, 200, 500]\n",
    "log_base = 10  # Or np.e for natural log\n",
    "\n",
    "# Specify channels (same as your second code snippet)\n",
    "strain_channel = 0\n",
    "aux_channel_indices_all = list(range(1, train_data_2d.shape[1]))\n",
    "\n",
    "n_aux_desired = 8#len(aux_channel_indices_all)\n",
    "aux_channel_indices = aux_channel_indices_all[:n_aux_desired]\n",
    "\n",
    "\n",
    "def set_frequency_ticks(ax, f_range, desired_ticks, log_base, new_height):\n",
    "    \"\"\"Sets the y-axis (frequency) ticks and labels.\"\"\"\n",
    "    log_f_range = (np.log(f_range[0]) / np.log(log_base), np.log(f_range[1]) / np.log(log_base))\n",
    "    log_desired_ticks = np.log(desired_ticks) / np.log(log_base)\n",
    "\n",
    "    y_ticks_pixel = np.interp(log_desired_ticks, log_f_range, [new_height - 1, 0])\n",
    "\n",
    "    y_ticks_pixel = [int(p) for p in y_ticks_pixel]\n",
    "    y_ticks_pixel = np.clip(y_ticks_pixel, 0, new_height - 1)\n",
    "\n",
    "    y_ticks_pixel, unique_indices = np.unique(y_ticks_pixel, return_index=True)\n",
    "    desired_ticks_used = np.array(desired_ticks)[unique_indices].tolist()\n",
    "\n",
    "    ax.grid(True, axis='y', which='both')\n",
    "    ax.set_yticks(y_ticks_pixel)\n",
    "    ax.set_yticklabels(desired_ticks_used)\n",
    "\n",
    "# ... (rest of the code is very similar, with modifications for ticks)\n",
    "\n",
    "for i in range(10):\n",
    "    print('---------------------------')\n",
    "    print(f'IMAGE {i}')\n",
    "\n",
    "    qplt_strain = torch.flipud(train_data_2d[i, strain_channel, f_min:f_max, t_min:t_max])\n",
    "\n",
    "    aux_images = []\n",
    "    for idx in aux_channel_indices:\n",
    "        aux_img = torch.flipud(train_data_2d[i, idx, :, :])\n",
    "        aux_images.append(aux_img)\n",
    "\n",
    "    total_plots = 1 + len(aux_channel_indices)\n",
    "    ncols = min(total_plots, 3)\n",
    "    nrows = math.ceil(total_plots / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "\n",
    "    if nrows * ncols == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # --- Plot Strain ---\n",
    "    im = axes[0].imshow(qplt_strain, aspect='auto', vmin=0, vmax=v_max)\n",
    "    axes[0].set_title('Strain')\n",
    "    axes[0].set_xlabel('Time (s)')  # Added units\n",
    "    axes[0].set_ylabel('Frequency (Hz)') # Added units\n",
    "    fig.colorbar(im, ax=axes[0])\n",
    "\n",
    "    set_frequency_ticks(axes[0], f_range, desired_ticks, log_base, qplt_strain.shape[0]) # Set ticks!\n",
    "    axes[0].set_xticks([t_min,(t_max-t_min)/2,t_max])\n",
    "    axes[0].set_xticklabels([0,0.5,1])\n",
    "\n",
    "\n",
    "    # --- Plot each aux channel ---\n",
    "    for j, aux_img in enumerate(aux_images):\n",
    "        ax = axes[j + 1]\n",
    "        im = ax.imshow(aux_img, aspect='auto', vmin=0, vmax=v_max)\n",
    "        ax.set_title(f'Aux {aux_channel_indices[j]}')\n",
    "        ax.set_xlabel('Time (s)') # Added units\n",
    "        ax.set_ylabel('Frequency (Hz)') # Added units\n",
    "        fig.colorbar(im, ax=ax)\n",
    "\n",
    "        set_frequency_ticks(ax, f_range, desired_ticks, log_base, aux_img.shape[0]) # Set ticks!\n",
    "        ax.set_xticks([t_min,(t_max-t_min)/2,t_max])\n",
    "        ax.set_xticklabels([0,0.5,1])\n",
    "\n",
    "\n",
    "    for k in range(total_plots, nrows * ncols):\n",
    "        axes[k].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalise Data & Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains different strategies to normalise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clamp Max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy consists in saturating and normalising the data to a certain  SNR^2 value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value=10000\n",
    "train_data_2d_clamp=torch.clamp(train_data_2d, min=0,max=max_value)\n",
    "test_data_2d_clamp=torch.clamp(test_data_2d, min=0,max=max_value)\n",
    "try:\n",
    "    background_tensor_clamp=torch.clamp(background_tensor, min=0,max=max_value)\n",
    "except:\n",
    "    print('No background tensor')\n",
    "\n",
    "#train_data_2d_norm/=max_value\n",
    "#test_data_2d_norm/=max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalize mean of channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Filter below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rows_below_threshold(data, threshold):\n",
    "    \"\"\"\n",
    "    Filters rows in the data tensor where all channels are below a certain threshold.\n",
    "\n",
    "    Input:\n",
    "    - data (torch.Tensor): dataset\n",
    "    - threshold (torch.Tensor): threshold value for each channel\n",
    "\n",
    "    Return:\n",
    "    - filtered_data (torch.Tensor): filtered dataset\n",
    "    \"\"\"\n",
    "    # Calculate the maximum value for each channel across all examples\n",
    "    max_vals = data.view(data.shape[0], data.shape[1], -1).max(-1)[0]\n",
    "    print(max_vals.shape)\n",
    "    print(threshold.unsqueeze(0).shape)\n",
    "    # Check if all three values in each row are below the respective threshold\n",
    "    mask = (max_vals < threshold.unsqueeze(0)).all(dim=1)\n",
    "    print(mask.shape)\n",
    "    \n",
    "    # Use the boolean mask to filter and keep only the rows in the dataset that satisfy the condition\n",
    "    filtered_data = data[mask]\n",
    "\n",
    "    return filtered_data,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_train_2d_below,mask_train=filter_rows_below_threshold(train_data_2d_clamp,torch.tensor([6,max_value,max_value,max_value,max_value,max_value,max_value,max_value,max_value]))\n",
    "filtered_data_test_2d_below, mask_test=filter_rows_below_threshold(test_data_2d_clamp,torch.tensor([6,max_value,max_value,max_value,max_value,max_value,max_value,max_value,max_value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_data_train_2d_below.shape)\n",
    "print(filtered_data_test_2d_below.shape)\n",
    "#print(filtered_data_background_2d_below.shape)\n",
    "background=torch.cat((filtered_data_train_2d_below,filtered_data_test_2d_below))\n",
    "background.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Filter above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rows_above_threshold(data, threshold):\n",
    "    \"\"\"\n",
    "    Filters rows in the data tensor where all channels are below a certain threshold.\n",
    "\n",
    "    Input:\n",
    "    - data (torch.Tensor): dataset\n",
    "    - threshold (torch.Tensor): threshold value for each channel\n",
    "\n",
    "    Return:\n",
    "    - filtered_data (torch.Tensor): filtered dataset\n",
    "    \"\"\"\n",
    "    # Calculate the maximum value for each channel across all examples\n",
    "    max_vals = data.view(data.shape[0], data.shape[1], -1).max(-1)[0]\n",
    "    print(max_vals.shape)\n",
    "    print(threshold.unsqueeze(0).shape)\n",
    "    # Check if all three values in each row are below the respective threshold\n",
    "    mask = (max_vals >= threshold.unsqueeze(0)).all(dim=1)\n",
    "    print(mask.shape)\n",
    "    \n",
    "    # Use the boolean mask to filter and keep only the rows in the dataset that satisfy the condition\n",
    "    filtered_data = data[mask]\n",
    "\n",
    "    return filtered_data,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_train_2d,mask_train_above=filter_rows_above_threshold(train_data_2d_clamp,torch.tensor([10,0,0,0,0,0,0,0,0]))\n",
    "filtered_data_test_2d, mask_test_above=filter_rows_above_threshold(test_data_2d_clamp,torch.tensor([10,0,0,0,0,0,0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_data_train_2d.shape)\n",
    "print(filtered_data_test_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stats and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(data):\n",
    "    print(data.shape)\n",
    "    \"\"\"\n",
    "    Normalizes the qplot data to the range [0,1] for NN convergence purposes\n",
    "    \n",
    "    Input:\n",
    "    - data (torch.Tensor) : dataset of qtransforms\n",
    "    \n",
    "    Return:\n",
    "    - data (torch.tensor) : normalized dataset\n",
    "    \"\"\"\n",
    "    max_vals = data.view(data.shape[0], data.shape[1], -1).max(-1)[0]  # Compute the maximum value for each 128x128 tensor\n",
    "    max_global = data.view(data.shape[0], data.shape[1], -1).max(0)[0].max(1)[0]\n",
    "    print(max_global)\n",
    "    print(\"Maximum value for each element tensor:\", max_vals.shape)\n",
    "    max_vals = max_vals.unsqueeze(-1).unsqueeze(-1)  # Add dimensions to match the shape of data for broadcasting\n",
    "    return max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfiltered data\n",
    "#max_train = find_max(train_data_2d_clamp)\n",
    "#max_test = find_max(test_data_2d_clamp)\n",
    "\n",
    "#Filtered data\n",
    "max_train = find_max(filtered_data_train_2d)\n",
    "max_test = find_max(filtered_data_test_2d)\n",
    "\n",
    "\n",
    "# Flatten the tensor along the channel dimension\n",
    "flattened_tensor = max_train.view(-1, num_aux_channels+1)\n",
    "flattened_tensor_test = max_test.view(-1, num_aux_channels+1)\n",
    "\n",
    "# Convert tensor to numpy array\n",
    "numpy_array = flattened_tensor.numpy()\n",
    "numpy_array_test= flattened_tensor_test.numpy()\n",
    "\n",
    "# Define custom bins\n",
    "custom_bins = [5, 10,15, 20, 50, 100,200,500,1000,np.inf]\n",
    "\n",
    "# Define the number of rows needed (3 subplots per row)\n",
    "num_channels = num_aux_channels + 1  # Including the main channel\n",
    "num_rows = math.ceil(num_channels / 3)  # Total rows needed\n",
    "\n",
    "# Plot histograms for each channel dimension using custom bins\n",
    "plt.figure(figsize=(12, num_rows * 4))  # Adjusting figure size dynamically\n",
    "\n",
    "for i in range(num_channels):\n",
    "    plt.subplot(num_rows, 3, i + 1)  # Arrange plots in num_rows x 3 grid\n",
    "    counts, bins, _ = plt.hist(numpy_array[:, i], bins=custom_bins, color='skyblue', alpha=0.7, histtype='barstacked')\n",
    "    plt.title(f'Channel {i+1} Histogram')\n",
    "    plt.xlabel('Value')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Display counts on the histogram bars\n",
    "    for count, bin_edge in zip(counts, bins[:-1]):  # Exclude last bin_edge\n",
    "        if count > 0:  # Avoid placing labels on empty bins\n",
    "            plt.text(bin_edge + (bins[1] - bins[0]) / 2, count, str(int(count)), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Calculate and print the average of each channel\n",
    "channel_means = np.mean(numpy_array, axis=0)\n",
    "channel_means_test = np.mean(numpy_array_test, axis=0)\n",
    "channel_std = np.std(numpy_array, axis=0)\n",
    "channel_std_test = np.std(numpy_array_test, axis=0)\n",
    "\n",
    "#Calculate and print the standard deviation of each channel\n",
    "\n",
    "print('TRAIN')\n",
    "for i, mean in enumerate(channel_means):\n",
    "    print(f'Average of Channel {i+1} train: {mean}')\n",
    "    print(f'std of Channel {i+1} train: {channel_std[i]}')\n",
    "    print(f'-----------------------------------------')\n",
    "print('\\n\\n TEST')   \n",
    "for i, mean in enumerate(channel_means_test):\n",
    "    print(f'Average of Channel {i+1} test: {mean}')\n",
    "    print(f'STD of Channel {i+1} train: {channel_std_test[i]}')\n",
    "    print(f'-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sum=0\n",
    "for count in counts:\n",
    "    print(count)\n",
    "    count_sum+=count\n",
    "print(count_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ch_mean(data, channel_means, channel_std=None):\n",
    "    \"\"\"\n",
    "    Normalizes the data by dividing each channel by its respective mean value,\n",
    "    or by subtracting the mean and dividing by the standard deviation if channel_std is provided.\n",
    "\n",
    "    Input:\n",
    "    - data (torch.Tensor): dataset\n",
    "    - channel_means (list or torch.Tensor): list of mean values for each channel\n",
    "    - channel_std (list or torch.Tensor, optional): list of standard deviation values for each channel. Defaults to None.\n",
    "\n",
    "    Return:\n",
    "    - normalized_data (torch.Tensor): normalized dataset\n",
    "    \"\"\"\n",
    "    # Convert channel_means and channel_std to tensors if they're not already\n",
    "    if not isinstance(channel_means, torch.Tensor):\n",
    "        channel_means = torch.tensor(channel_means)\n",
    "    if channel_std is not None and not isinstance(channel_std, torch.Tensor):\n",
    "        channel_std = torch.tensor(channel_std)\n",
    "\n",
    "\n",
    "    # Check if channel_means has the correct shape\n",
    "    if channel_means.shape[0] != data.shape[1]:\n",
    "        raise ValueError(\"Number of elements in channel_means must match the number of channels in data.\")\n",
    "\n",
    "    # Reshape channel_means and channel_std to match the shape of data for broadcasting\n",
    "    channel_means = channel_means.view(1, -1, 1, 1)\n",
    "    if channel_std is not None:\n",
    "        if channel_std.shape[0] != data.shape[1]:\n",
    "            raise ValueError(\"Number of elements in channel_std must match the number of channels in data.\")\n",
    "        channel_std = channel_std.view(1, -1, 1, 1)\n",
    "\n",
    "    # Normalize data\n",
    "    if channel_std is None:\n",
    "        normalized_data = data / channel_means\n",
    "    else:\n",
    "        normalized_data = (data - channel_means) / channel_std\n",
    "\n",
    "    return normalized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_means.shape\n",
    "norm_factor=torch.tensor(channel_means[0]).unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfiltered data\n",
    "#train_data_2d_norm=normalize_ch_mean(train_data_2d_clamp,channel_means) #,channel_std\n",
    "#test_data_2d_norm=normalize_ch_mean(test_data_2d_clamp,channel_means)  #,channel_means,channel_std # not channel_means_test, it should be the same as train data\n",
    "\n",
    "#Filtered data\n",
    "train_data_2d_norm=normalize_ch_mean(filtered_data_train_2d,channel_means) #,channel_std\n",
    "test_data_2d_norm=normalize_ch_mean(filtered_data_test_2d,channel_means)  #,channel_means,channel_std # not channel_means_test, it should be the same as train data\n",
    "background_norm=normalize_ch_mean(background,channel_means)  #,channel_means,channel_std # not channel_means_test, it should be the same as train data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 #200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create dataloader objects with preprocessed dataset\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_data_2d_norm,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data_2d_norm,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_background_dataloader = DataLoader(\n",
    "    background_norm,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define different NN architectures models, and initialise one of them as the generator to use in training and inference.\n",
    "\n",
    "This section is split in three parts:\n",
    "- 1) **Weight Initialization**, where we define the function to initialise the weights of the NN models according to certain parameters and distributions passed as input\n",
    "- 2) **NN Models**, where we define different NN models exploting different architecutres\n",
    "- 3) **Generator**, where we initialise one of the above models as the generator to use in training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section we define a NN model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Unet with residual blocks and attention gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            #nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, 1, bias=False) if in_channels != out_channels else nn.Identity()\n",
    "        self.activation = nn.LeakyReLU(0.01, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels//8, 1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels//8, 1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        bs, c, h, w = x.size()\n",
    "        proj_query = self.query(x).view(bs, -1, h*w).permute(0,2,1)\n",
    "        proj_key = self.key(g).view(bs, -1, h*w)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value(g).view(bs, -1, h*w)\n",
    "        \n",
    "        out = torch.bmm(proj_value, attention.permute(0,2,1))\n",
    "        out = out.view(bs, c, h, w)\n",
    "        return self.gamma * out + x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=10, output_channels=1, base_channels=64, use_attention=True,encoder_dropout_rate=0.2,decoder_dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.use_attention = use_attention\n",
    "        self._initialize_weights()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            ResidualBlock(input_channels, base_channels,dropout_rate=encoder_dropout_rate),\n",
    "            ResidualBlock(base_channels, base_channels,dropout_rate=encoder_dropout_rate)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            ResidualBlock(base_channels, base_channels*2,dropout_rate=encoder_dropout_rate),\n",
    "            ResidualBlock(base_channels*2, base_channels*2,dropout_rate=encoder_dropout_rate)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            ResidualBlock(base_channels*2, base_channels*4,dropout_rate=encoder_dropout_rate),\n",
    "            ResidualBlock(base_channels*4, base_channels*4,dropout_rate=encoder_dropout_rate)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ResidualBlock(base_channels*4, base_channels*8,dropout_rate=decoder_dropout_rate),\n",
    "            ResidualBlock(base_channels*8, base_channels*8,dropout_rate=decoder_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Decoder with or without attention\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(base_channels*8, base_channels*4, 3, padding=1)\n",
    "        )\n",
    "        if self.use_attention:\n",
    "            self.att1 = AttentionGate(base_channels*4)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            ResidualBlock(base_channels*8 if self.use_attention else base_channels*4, base_channels*4,dropout_rate=decoder_dropout_rate), # Conditional input channels\n",
    "            ResidualBlock(base_channels*4, base_channels*4,dropout_rate=encoder_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(base_channels*4, base_channels*2, 3, padding=1)\n",
    "        )\n",
    "        if self.use_attention:\n",
    "            self.att2 = AttentionGate(base_channels*2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            ResidualBlock(base_channels*4 if self.use_attention else base_channels*2, base_channels*2,dropout_rate=decoder_dropout_rate), # Conditional input channels\n",
    "            ResidualBlock(base_channels*2, base_channels*2,dropout_rate=decoder_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(base_channels*2, base_channels, 3, padding=1)\n",
    "        )\n",
    "        if self.use_attention:\n",
    "            self.att3 = AttentionGate(base_channels)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            ResidualBlock(base_channels*2 if self.use_attention else base_channels, base_channels,dropout_rate=decoder_dropout_rate), # Conditional input channels\n",
    "            ResidualBlock(base_channels, base_channels,dropout_rate=decoder_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, output_channels, 1),\n",
    "            nn.Softplus() # Output is positive semidefinite\n",
    "        )\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool3(e3))\n",
    "        \n",
    "        # Decoder\n",
    "        d1 = self.up1(b)\n",
    "        if self.use_attention:\n",
    "            e3 = self.att1(e3, d1)\n",
    "            d1 = self.dec1(torch.cat([d1, e3], 1))\n",
    "        else:\n",
    "            d1 = self.dec1(d1) # No attention, direct input\n",
    "\n",
    "        d2 = self.up2(d1)\n",
    "        if self.use_attention:\n",
    "            e2 = self.att2(e2, d2)\n",
    "            d2 = self.dec2(torch.cat([d2, e2], 1))\n",
    "        else:\n",
    "            d2 = self.dec2(d2) # No attention, direct input\n",
    "        \n",
    "        d3 = self.up3(d2)\n",
    "        if self.use_attention:\n",
    "            e1 = self.att3(e1, d3)\n",
    "            d3 = self.dec3(torch.cat([d3, e1], 1))\n",
    "        else:\n",
    "            d3 = self.dec3(d3) # No attention, direct input\n",
    "        \n",
    "        return self.final(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels=num_aux_channels\n",
    "output_channels=1\n",
    "\n",
    "try:\n",
    "    del generator_2d\n",
    "    print('generator deleted')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "generator_2d = UNet(\n",
    "    input_channels=input_channels,\n",
    "    output_channels=output_channels,\n",
    "    base_channels=64, # Keep channel specification\n",
    "    use_attention=True \n",
    ").to(device)\n",
    "print(generator_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(generator_2d, input_size=(batch_size, num_aux_channels,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we train the previously defined and initialised NN model.\n",
    "\n",
    "This section is divided into three parts:\n",
    "- 1) **Functions**, which contains utils functions to calculate several loss functions for the networks, a metric for accuracy (not used in the current version of the notebook) a function to make inference and a function to train the model and save the weights\n",
    "- 2) **Pre-training generation**, where we make inference on test data using untrained network\n",
    "- 3) **Actual training**, where we train the NN, save the weigths and plot losses curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_iou_2d_non0(generated_tensor, target_tensor, threshold=20/channel_means[0]):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) in the 2D plane at the specified intensity threshold for each element in the batch.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_tensor: Tensor containing generated spectrograms (batch_size x 1 x height x width)\n",
    "    - target_tensor: Tensor containing target spectrograms (batch_size x 1 x height x width)\n",
    "    - threshold: Intensity threshold for determining the binary masks\n",
    "\n",
    "    Returns:\n",
    "    - mean_iou: Mean Intersection over Union (IoU) across all elements in the batch\n",
    "    - zero_union_count: Count of elements in the batch with a union of 0\n",
    "    \"\"\"\n",
    "    # Convert intensity threshold to tensor\n",
    "    threshold_tensor = torch.tensor(threshold, device=generated_tensor.device)\n",
    "\n",
    "    # Create binary masks based on the intensity threshold\n",
    "    gen_mask = generated_tensor >= threshold_tensor\n",
    "    tgt_mask = target_tensor >= threshold_tensor\n",
    "\n",
    "    # Convert masks to float tensors\n",
    "    gen_mask = gen_mask.float()\n",
    "    tgt_mask = tgt_mask.float()\n",
    "\n",
    "    \n",
    "    # Calculate intersection and union for each element in the batch\n",
    "    intersection = torch.sum(gen_mask * tgt_mask, dim=(1, 2, 3))\n",
    "    union = torch.sum(gen_mask, dim=(1, 2, 3)) + torch.sum(tgt_mask, dim=(1, 2, 3)) - intersection\n",
    "\n",
    "    # Find elements with union 0\n",
    "    zero_union_mask = union == 0\n",
    "    zero_union_count = torch.sum(zero_union_mask).item()\n",
    "\n",
    "    # Exclude elements with union 0 from the IoU calculation\n",
    "    iou = intersection / union\n",
    "    iou[zero_union_mask] = 0\n",
    "\n",
    "    # Take mean over non-zero elements in the batch\n",
    "    non_zero_count = len(union) - zero_union_count\n",
    "    mean_iou = torch.sum(iou) / non_zero_count if non_zero_count > 0 else 0\n",
    "    \n",
    "    # Count elements with IoU above 0.9\n",
    "    above_09_count = torch.sum(iou > 0.9).item()\n",
    "\n",
    "    return mean_iou#.item()#, zero_union_count, above_09_count\n",
    "    #return mean_iou.item(), zero_union_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils function to generate data using the decoder for inference \n",
    "def generate_data(generator, batch, normalize='Each'):\n",
    "    \"\"\"\n",
    "    Generate data using a generator model.\n",
    "\n",
    "    Args:\n",
    "        - generator (nn.Module): Generator model.\n",
    "        - batch (torch.Tensor): Input batch data.\n",
    "        - normalize (bool): Flag indicating whether to normalize the generated data (default is False).\n",
    "\n",
    "    Returns:\n",
    "        - torch.Tensor: Generated data.\n",
    "    \"\"\"\n",
    "    target = batch[:, 0].unsqueeze(1).to(device)\n",
    "    input = batch[:, 1:].to(device)\n",
    "    with torch.no_grad():\n",
    "        generated = generator(input.float())\n",
    "        if normalize=='Each':\n",
    "            print(generated.shape)\n",
    "            generated = normalize_each(generated)\n",
    "        elif normalize=='Column':\n",
    "            print(generated.shape)\n",
    "            generated = normalize_(generated, 1)\n",
    "     \n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder(num_epochs, generator, criterion1, optimizer, dataloader, val_loader, accuracy, checkpoint_path, \n",
    "                  save_best=True, scheduler=None, switch_threshold=0.01, patience=3):\n",
    "    \"\"\"\n",
    "    Trains the generator model using a combination of two loss functions with dynamic weighting.\n",
    "\n",
    "    Args:\n",
    "        num_epochs: (int) Number of epochs for training.\n",
    "        generator: (NN.Module) NN model to train.\n",
    "        criterion1: (CustomLoss) Primary loss function (e.g., L1).\n",
    "        criterion2: (CustomLoss) Secondary loss function (e.g., L2).\n",
    "        optimizer: (torch.optim) Optimizer for training.\n",
    "        dataloader: (DataLoader) Training data loader.\n",
    "        val_loader: (DataLoader) Validation data loader.\n",
    "        accuracy: (function) Metric to measure performance of the model.\n",
    "        checkpoint_path: (str) Path to save checkpoints.\n",
    "        save_best: (bool) Whether to save the best performing model.\n",
    "        scheduler: (torch.optim.lr_scheduler) Learning rate scheduler.\n",
    "        switch_threshold: (float) Minimum change in loss to consider as progress.\n",
    "        patience: (int) Number of epochs to wait before switching loss priority.\n",
    "    \n",
    "    Returns:\n",
    "        loss_plot, val_loss_plot: Training and validation loss history.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize tracking metrics\n",
    "    loss_plot = []\n",
    "    val_loss_plot = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        print(f'{scheduler=}')\n",
    "    \n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        generator.train()  # Set model to training mode\n",
    "        epoch_loss = []\n",
    "\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            torch.cuda.empty_cache()\n",
    "            target = batch[:, 0].unsqueeze(1).to(device).float()\n",
    "            input = batch[:, 1:].to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            generated = generator(input.float())  # Forward pass\n",
    "\n",
    "            # Compute both loss components\n",
    "            total_loss = criterion1(generated, target)\n",
    "\n",
    "\n",
    "            \n",
    "            total_loss.backward()  # Backpropagation\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=5.0)\n",
    "            \n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            epoch_loss.append(total_loss.detach().cpu().numpy())\n",
    "\n",
    "        # Validation phase\n",
    "        generator.eval()  # Set model to evaluation mode\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                torch.cuda.empty_cache()\n",
    "                target = batch[:, 0].unsqueeze(1).to(device).float()\n",
    "                input = batch[:, 1:].to(device)\n",
    "                generated = generator(input.float())\n",
    "\n",
    "                # Compute validation losses\n",
    "                total_val_loss = criterion1(generated, target)     \n",
    "                 \n",
    "                val_loss.append(total_val_loss.detach().cpu().numpy())\n",
    "\n",
    "        # Record training and validation loss\n",
    "        loss_plot.append(np.mean(epoch_loss))\n",
    "        val_loss_plot.append(np.mean(val_loss))\n",
    "        \n",
    "        # Adjust learning rate using scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss_plot[-1])\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch}: training loss {loss_plot[-1]:.4e}, val loss {val_loss_plot[-1]:.4e}')\n",
    "\n",
    "        # Improvement check (check if the loss has stagnated)\n",
    "        if epoch > 1:\n",
    "            improvement = (val_loss_plot[-2] - val_loss_plot[-1]) / val_loss_plot[-2]\n",
    "            print(f'Improvement: {improvement*100:.4f}%')\n",
    "\n",
    "\n",
    "        # Save checkpoint if validation loss improves\n",
    "        if save_best and val_loss_plot[-1] < best_val_loss:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': generator.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss_plot[-1],\n",
    "                'val_loss': val_loss_plot[-1],\n",
    "            }\n",
    "            best_val_loss = val_loss_plot[-1]\n",
    "            torch.save(checkpoint, checkpoint_path.format('best'))\n",
    "            \n",
    "        # Evaluate accuracy every 10 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            total_accuracy = 0\n",
    "            for batch in val_loader:\n",
    "                target = batch[:, 0].unsqueeze(1).to(device).float()\n",
    "                input = batch[:, 1:].to(device)\n",
    "                generated = generator(input.float())\n",
    "                total_accuracy += accuracy(generated, target).detach().cpu()\n",
    "                torch.cuda.empty_cache()\n",
    "            avg_accuracy = total_accuracy / len(val_loader)\n",
    "            print(f'Epoch {epoch}: Validation accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "    return loss_plot, val_loss_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pre-training generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in(tqdm(test_dataloader)):\n",
    "    generated=generate_data(generator_2d,batch,normalize=False)\n",
    "    break\n",
    "generated[1,0].shape\n",
    "#batch=transform(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qplt_g=generated[1,0].detach().cpu().numpy()\n",
    "qplt_r=batch[1,0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qplt_g.shape)\n",
    "qplt_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(qplt_g, aspect='auto',vmin=0,vmax=1)\n",
    "plt.title('Generated - pre training')\n",
    "plt.xlabel('Time [pixel]')\n",
    "plt.ylabel('Frequency [pixel]')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre training loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAbsDiff(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanAbsDiff, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.abs(y_pred - y_true)\n",
    "        return loss.mean()\n",
    "\n",
    "class StdAbsDiff(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StdAbsDiff, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.abs(y_pred - y_true)\n",
    "        return loss.std()\n",
    "\n",
    "metric_mean = MeanAbsDiff()\n",
    "metric_std = StdAbsDiff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_single_loss(generator, criterion, val_loader):\n",
    "    generator.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    val_total_loss = []  # To store total losses\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            torch.cuda.empty_cache()\n",
    "            target = batch[:, 0].unsqueeze(1).to(device)\n",
    "            input_ = batch[:, 1:].to(device)\n",
    "            generated = generator(input_)\n",
    "\n",
    "            # Get the individual loss components from the criterion\n",
    "            total_loss = criterion(generated, target)\n",
    "\n",
    "            val_total_loss.append(total_loss.item())\n",
    "\n",
    "    # Return mean of the losses and the full lists\n",
    "\n",
    "    mean_total_loss = np.mean(val_total_loss)\n",
    "\n",
    "    return mean_total_loss, val_total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, data_range=21.0):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.ssim = StructuralSimilarityIndexMeasure(data_range=data_range).to(device)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        ssim_loss = 1 - self.ssim(pred, target)\n",
    "        return self.alpha * l1 + (1 - self.alpha) * ssim_loss\n",
    "loss=CustomLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_loss,val_total_loss=calculate_single_loss(generator_2d,loss,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_metric,val_total_metric=calculate_single_loss(generator_2d,metric_mean,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_iou,val_total_iou=calculate_single_loss(generator_2d,calculate_iou_2d_non0,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(np.flipud(qplt_r), aspect='auto',vmin=0,vmax=0.5)\n",
    "plt.title('Real')\n",
    "plt.ylabel('Time [pixel]')\n",
    "plt.xlabel('Frequency [pixel]')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# learning rate, and optimiser\n",
    "\n",
    "\n",
    "lr=1.0e-4\n",
    "\n",
    "#lr=0.001\n",
    "\n",
    "momentum=0.9\n",
    "\n",
    "#G_optimizer = torch.optim.Adam(generator_2d.parameters(), lr=lr, weight_decay=1e-5 )\n",
    "G_optimizer = torch.optim.AdamW(generator_2d.parameters(), \n",
    "                            lr=lr, \n",
    "                            weight_decay=1e-4,  # Critical for generalization\n",
    "                            betas=(0.9, 0.999))\n",
    "#G_optimizer = torch.optim.AdamW(generator_2d.parameters(), lr=lr )\n",
    "\n",
    "#G_optimizer = torch.optim.AdamW(generator_2d.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "#G_optimizer = torch.optim.SGD(generator_2d.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    G_optimizer, \n",
    "    mode='min', \n",
    "    patience=7,  # Increased from 5\n",
    "    factor=0.5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_name='Unet_with_residualblocks_64x64_l1_SSIM_norm_maxmean_no_whiten_8-500Hz_logf_9channels'\n",
    "save_checkpoint='/home/jovyan/Resnet_Qtransform/'+save_name+'.checkpoint_epoch_{}.pth'\n",
    "n_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_plot, val_loss_plot=train_decoder(n_epochs,generator_2d,loss,G_optimizer,dataloader,test_dataloader,calculate_iou_2d_non0,save_checkpoint,scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_plot,color='b',label='train')\n",
    "plt.plot(val_loss_plot,color='r',label='validation')\n",
    "plt.title('L1 loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'{save_name}_Loss.pdf')\n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we make inference on test dataset using trained NN, and we plot the generated qplots.\n",
    "\n",
    "This section is devided in two parts:\n",
    "- 1) **Load Model**, where we load the model from checkpoint\n",
    "- 2) **Actual Inferece**, where we generate data for main channel from the test dataset. We also plot the generated data and compare it to the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "#load_path='/home/jovyan/ResNet_SNR_above_20-30-30.checkpoint_epoch_best.pth'\n",
    "#load_path='/home/jovyan/ResNet_SNR_above_15.checkpoint_epoch_best.pth' \n",
    "#load_path='/home/jovyan/ResNet_15_channels.checkpoint_epoch_best.pth'\n",
    "\n",
    "#save_name='Unet_with_residualblocks_64x64_l1_SSIM_norm_maxmean_weight_decay1e-4_adamW_batch200_lr1e-4_no_whiten_8-500Hz_logf'\n",
    "#save_checkpoint='/home/jovyan/Resnet_Qtransform/'+save_name+'.checkpoint_epoch_{}.pth'\n",
    "load_path=save_checkpoint.format('best')\n",
    "\n",
    "checkpoint = torch.load(load_path)\n",
    "generator_2d.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Actual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make inference on test data\n",
    "\n",
    "for batch in(tqdm(test_dataloader)):\n",
    "    generated_post=generate_data(generator_2d,batch,normalize=False)\n",
    "    break\n",
    "generated_post[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qplt_g=generated_post[0,0].detach().cpu().numpy()\n",
    "qplt_r=batch[0,0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_mean=MeanAbsDiff()\n",
    "metric_std=StdAbsDiff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_loss_train,train_total_loss=calculate_single_loss(generator_2d,metric_mean,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_loss_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Real, Generated and input Qplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape\n",
    "plt.figure(figsize= (6, 6))\n",
    "plt.imshow(batch[5,0], aspect='auto',vmin=0,vmax=1)\n",
    "plt.title('Real')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in(tqdm(test_dataloader)):\n",
    "    print(batch.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "v_max = 25\n",
    "\n",
    "def plot_images(generated_post, batch, channel_means, num_aux_channels=8,num_images=10):\n",
    "    for i in range(num_images):\n",
    "        print('---------------------------')\n",
    "        print(f'IMAGE {i}')\n",
    "\n",
    "        qplt_g = torch.flipud(generated_post[i, 0].detach().cpu() * channel_means[0])\n",
    "        qplt_r = torch.flipud(batch[i, 0].detach().cpu() * channel_means[0])\n",
    "\n",
    "        time_extent = generated_post[i, 0].shape[0]\n",
    "        freq_extent = generated_post[i, 0].shape[1]\n",
    "        extent = [0, time_extent, 0, freq_extent]\n",
    "\n",
    "        num_rows_aux = (num_aux_channels + 3) // 4\n",
    "\n",
    "        fig, axes = plt.subplots(1 + num_rows_aux, 4, figsize=(20, 5 * (1 + num_rows_aux)))\n",
    "\n",
    "        # Handle the case where there's only one row (including 0 aux channels)\n",
    "        if 1 + num_rows_aux == 1:  # Only one row\n",
    "            axes = np.array([axes]) # make axes 2D so that it works with the rest of the code\n",
    "            axes = axes.reshape(1,4) # reshape it to be a 1x4 array\n",
    "\n",
    "        im_r = axes[0, 0].imshow(qplt_r, aspect='auto', extent=extent, vmin=0, vmax=v_max)\n",
    "        axes[0, 0].set_title('Real')\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        fig.colorbar(im_r, ax=axes[0, 0])\n",
    "\n",
    "        im_g = axes[0, 1].imshow(qplt_g, aspect='auto', extent=extent, vmin=0, vmax=v_max)\n",
    "        axes[0, 1].set_title('Generated')\n",
    "        axes[0, 1].set_xlabel('Time')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        fig.colorbar(im_g, ax=axes[0, 1])\n",
    "\n",
    "        im_diff = axes[0, 2].imshow(torch.abs(qplt_g - qplt_r), aspect='auto', extent=extent, vmin=0, vmax=v_max)\n",
    "        axes[0, 2].set_title('True - Generated')\n",
    "        axes[0, 2].set_xlabel('Time')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        fig.colorbar(im_diff, ax=axes[0, 2])\n",
    "\n",
    "        axes[0, 3].axis('off')\n",
    "\n",
    "        aux_channel_index = 1\n",
    "        row = 1\n",
    "        col = 0\n",
    "\n",
    "        for j in range(num_aux_channels):\n",
    "            qplt_aux = torch.flipud(batch[i, aux_channel_index].detach().cpu() * channel_means[aux_channel_index])\n",
    "            im_aux = axes[row, col].imshow(qplt_aux, aspect='auto', extent=extent, vmin=0, vmax=v_max)\n",
    "            axes[row, col].set_title(f'aux{aux_channel_index}')\n",
    "            axes[row, col].set_xlabel('Time')\n",
    "            axes[row, col].set_ylabel('Frequency')\n",
    "            fig.colorbar(im_aux, ax=axes[row, col])\n",
    "\n",
    "            aux_channel_index += 1\n",
    "            col += 1\n",
    "            if col == 4:\n",
    "                col = 0\n",
    "                row += 1\n",
    "\n",
    "        for r in range(row, 1 + num_rows_aux):\n",
    "            for c in range(4):\n",
    "                axes[r, c].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f'Inference.pdf')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot only the first 4 aux channels:\n",
    "plot_images(generated_post, batch, channel_means, num_aux_channels=0,num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Accuracy performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genrate data using NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tensor_pre = torch.tensor([]).to('cpu')  # Initialize an empty tensor\n",
    "for batch in tqdm(test_dataloader):\n",
    "    generated_post = generate_data(generator_2d, batch.detach().cpu(), normalize=False).to('cpu')\n",
    "    generated_tensor_pre = torch.cat((generated_tensor_pre, generated_post), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "background set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_tensor = torch.tensor([]).to('cpu')  # Initialize an empty tensor\n",
    "for batch in tqdm(test_background_dataloader):\n",
    "    background_post = generate_data(generator_2d, batch.detach().cpu()/norm_factor, normalize=False).to('cpu')\n",
    "    background_tensor = torch.cat((background_tensor, background_post), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_tensor.shape[0]/generated_tensor_pre.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tensor=torch.cat((generated_tensor_pre,background_tensor), dim=0)\n",
    "generated_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=torch.cat((torch.ones(generated_tensor_pre.shape[0]),torch.zeros(background_tensor.shape[0])))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor=torch.cat((test_data_2d_norm[:,0,:,:].unsqueeze(1),background[:,0,:,:].unsqueeze(1)/norm_factor))\n",
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define clustering NN for accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label\n",
    "\n",
    "class ClusterAboveThreshold(nn.Module):\n",
    "    def __init__(self, threshold, min_cluster_area):\n",
    "        super(ClusterAboveThreshold, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.min_cluster_area = min_cluster_area\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Create a boolean mask based on the threshold\n",
    "        mask = input_tensor.squeeze(1) >= self.threshold  # Squeeze the channel dimension\n",
    "        #for i in range(mask.shape[0]):\n",
    "            #print(torch.count_nonzero(mask[i]))\n",
    "        \n",
    "        # Label connected components for the entire batch\n",
    "        labeled_masks, num_features = label(mask.cpu().numpy(), connectivity=2, return_num=True)\n",
    "\n",
    "        \n",
    "        # Reshape labeled_masks to [batch_size, num_features, height, width]\n",
    "        labeled_masks = torch.tensor(labeled_masks, dtype=torch.long, device=input_tensor.device)\n",
    "        labeled_masks = labeled_masks.view(input_tensor.size(0), -1, input_tensor.size(-2), input_tensor.size(-1))\n",
    "        \n",
    "        # Give unique labels to each cluster acroos each item\n",
    "        labeled_masks_sorted=self.sort_labels(labeled_masks)\n",
    "        \n",
    "        batch_clusters=[]\n",
    "        for idx, item in enumerate(input_tensor):\n",
    "            item_clusters=[]\n",
    "            for cluster in torch.unique(labeled_masks_sorted[idx]):\n",
    "                if cluster==torch.tensor(0):\n",
    "                    continue\n",
    "                cluster_pixels = (labeled_masks_sorted[idx] == cluster)\n",
    "\n",
    "                # Compute the total area of the cluster\n",
    "                cluster_area = torch.sum(cluster_pixels)\n",
    "\n",
    "\n",
    "                # Check if the cluster area is greater than the threshold area\n",
    "                if cluster_area > self.min_cluster_area:\n",
    "                    # Flatten the tensor\n",
    "                    masked_item = item.masked_fill(~cluster_pixels, 0)\n",
    "                    flattened_tensor = masked_item.flatten()\n",
    "                    \n",
    "                    # Compute the maximum value and its index across all dimensions\n",
    "                    max_value, max_index_flat = torch.max(flattened_tensor, dim=0)\n",
    "\n",
    "                    # Unravel the flattened index to get the original indices\n",
    "                    max_index_unraveled = np.unravel_index(max_index_flat.item(), item.shape)\n",
    "                    item_clusters.append((max_value,max_index_unraveled[1],max_index_unraveled[2]))\n",
    "            \n",
    "            batch_clusters.append(item_clusters)   \n",
    "            \n",
    "\n",
    "        \n",
    "        return batch_clusters\n",
    "    \n",
    "    def sort_labels(self, labeled_masks):\n",
    "        # Rename clusters for each item in the batch\n",
    "        offset = 0\n",
    "        for i in range(labeled_masks.shape[0]):  # Iterate over batch dimension\n",
    "            item_labeled_masks = labeled_masks[i]  # Get labeled mask for the current item\n",
    "            unique_labels = torch.unique(item_labeled_masks)  # Get unique labels in the item's labeled mask\n",
    "            \n",
    "            # Exclude background class label (label 0)\n",
    "            unique_labels = unique_labels[unique_labels != 0]\n",
    "\n",
    "\n",
    "            # Rename the labels with an offset, starting from 1\n",
    "            renamed_labels = item_labeled_masks.clone()\n",
    "            for j, label in enumerate(unique_labels, start=1):\n",
    "                mask = item_labeled_masks == label\n",
    "                renamed_labels[mask] = j + offset\n",
    "\n",
    "            # Update the offset for the next item\n",
    "            offset += len(unique_labels)\n",
    "\n",
    "            # Update the labeled mask for the current item\n",
    "            labeled_masks[i] = renamed_labels\n",
    "\n",
    "        # labeled_masks now contains the labeled masks with renamed clusters\n",
    "        return labeled_masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define normalisation factors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_factor=torch.tensor(channel_means[0]).unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "norm_factor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Tensors for accuracy check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalised data\n",
    "abs_difference_tensor=torch.abs((generated_tensor-target_tensor)*norm_factor)\n",
    "abs_difference_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_empty_lists(list_of_lists):\n",
    "    # Count the number of non-empty lists\n",
    "    non_empty_count = sum(1 for sublist in list_of_lists if sublist)\n",
    "    \n",
    "    # Calculate the fraction\n",
    "    fraction =1- (non_empty_count / len(list_of_lists))\n",
    "    \n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glitch_classifier(list_of_lists):\n",
    "    list=[1 if sublist else 0 for sublist in list_of_lists ]\n",
    "    #print(len(list))\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_accuracy(predictions,labels):\n",
    "    list_check=[(x + y)%2 for x, y in zip(predictions, labels)]\n",
    "    list_check=np.array(list_check)\n",
    "    accuracy=1-np.mean(list_check)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(predictions,labels):\n",
    "    cm={}\n",
    "    for x,y in zip(predictions,labels):\n",
    "        if x==0:\n",
    "            if y==0:\n",
    "                cm['TN'] = cm.get('TN', 0) + 1\n",
    "            elif y==1:\n",
    "                cm['TP'] = cm.get('TP', 0) + 1\n",
    "        elif x==1:\n",
    "            if y==0:\n",
    "                cm['FP'] = cm.get('FP', 0) + 1\n",
    "            elif y==1:\n",
    "                cm['FN'] = cm.get('FN', 0) + 1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def roc_curve(generated,labels,threshold_set=(10,20.1,0.1),min_cluster_area=10):\n",
    "    \n",
    "    roc_dict={}\n",
    "    for threshold in tqdm(np.arange(threshold_set[0],threshold_set[1],threshold_set[2])):\n",
    "        try:\n",
    "            del cluster_nn\n",
    "        except:\n",
    "            pass\n",
    "        cluster_nn= ClusterAboveThreshold(threshold, min_cluster_area).to('cpu')  \n",
    "        predictions=glitch_classifier(cluster_nn(generated))\n",
    "        cm=confusion_matrix(predictions,labels)\n",
    "        roc_dict[threshold]=cm\n",
    "    return roc_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming ClusterAboveThreshold, abs_difference_tensor, generated_tensor, target_tensor, and norm_factor are defined elsewhere\n",
    "\n",
    "def analyze_clusters_for_thresholds(abs_difference_tensor, generated_tensor, target_tensor, norm_factor, min_cluster_area=1):\n",
    "    \"\"\"\n",
    "    Analyzes cluster data for a range of threshold values.\n",
    "\n",
    "    Args:\n",
    "        abs_difference_tensor: Tensor of absolute differences.\n",
    "        generated_tensor: Tensor of generated data.\n",
    "        target_tensor: Tensor of target data.\n",
    "        norm_factor: Normalization factor.\n",
    "        min_cluster_area: Minimum cluster area.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two lists:\n",
    "            - cluster_abs_diff_accuracies: List of classifier accuracies for abs_difference_tensor.\n",
    "            - clusters_generated_accuracies: List of classifier accuracies for generated_tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    cluster_abs_diff_accuracies = []\n",
    "    clusters_generated_accuracies = []\n",
    "\n",
    "    for threshold in tqdm(range(1, 51)):\n",
    "        try:\n",
    "            del cluster_nn\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # pipeline\n",
    "        cluster_nn = ClusterAboveThreshold(threshold, min_cluster_area).to('cpu')  # Assuming to('cpu') is needed\n",
    "\n",
    "        # get clusters\n",
    "        clusters_abs_diff = cluster_nn(abs_difference_tensor)\n",
    "        clusters_generated = cluster_nn(generated_tensor * norm_factor)\n",
    "        clusters_target = cluster_nn(target_tensor * norm_factor)\n",
    "\n",
    "\n",
    "        #set labels\n",
    "        target_labels = glitch_classifier(clusters_target)  # Use target clusters as labels\n",
    "        diff_labels= [0 for k in range(len(target_labels))]\n",
    "        \n",
    "        # Calculate classifier accuracy for abs_difference_tensor\n",
    "        abs_diff_predictions = glitch_classifier(clusters_abs_diff)\n",
    "        abs_diff_accuracy = classifier_accuracy(abs_diff_predictions, diff_labels)\n",
    "        cluster_abs_diff_accuracies.append(abs_diff_accuracy)\n",
    "\n",
    "        # Calculate classifier accuracy for generated_tensor\n",
    "        generated_predictions = glitch_classifier(clusters_generated)\n",
    "        generated_accuracy = classifier_accuracy(generated_predictions, target_labels)\n",
    "        clusters_generated_accuracies.append(generated_accuracy)\n",
    "\n",
    "    return cluster_abs_diff_accuracies, clusters_generated_accuracies\n",
    "\n",
    "\n",
    "# Example usage (replace with your actual data and ClusterAboveThreshold definition):\n",
    "# abs_difference_tensor = ...  # Your tensor data\n",
    "# generated_tensor = ...  # Your tensor data\n",
    "# target_tensor = ...  # Your tensor data\n",
    "# norm_factor = ...  # Your normalization factor\n",
    "\n",
    "cluster_abs_diff_accuracies, clusters_generated_accuracies = analyze_clusters_for_thresholds(abs_difference_tensor, generated_tensor, target_tensor, norm_factor\n",
    ")\n",
    "\n",
    "# print(\"Cluster Abs Diff Accuracies:\", cluster_abs_diff_accuracies)\n",
    "# print(\"Clusters Generated Accuracies:\", clusters_generated_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "thresholds = range(1, 51)  # The SNR^2 thresholds\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size for better visualization\n",
    "\n",
    "plt.plot(thresholds, cluster_abs_diff_accuracies, label=\"Denoising Accuracy\", marker='o', linestyle='-')\n",
    "plt.plot(thresholds[5:], clusters_generated_accuracies[5:], label=\"Vetoing Accuracy\", marker='x', linestyle='--')\n",
    "#plt.plot(thresholds, cluster_abs_diff_accuracies_veto, label=\"Vetoing Accuracy for veto correctly flagged data\", marker='p', linestyle='--')\n",
    "\n",
    "plt.xlabel(r\"$\\mathrm{SNR^2}$ Threshold\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "plt.title(\"Accuracy vs. $\\mathrm{SNR^2}$\", fontsize=22)\n",
    "plt.xticks(np.arange(min(thresholds), max(thresholds)+1, 5.0), fontsize=16) # set ticks every 5\n",
    "plt.yticks(fontsize=16)\n",
    "plt.grid(True)  # Add a grid for better readability\n",
    "plt.legend(fontsize=20)  # Show the legend\n",
    "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot successfully cleaned data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_of_empty_sublists(list_of_sublists):\n",
    "    # Initialize an empty list to store the indices\n",
    "    indices = []\n",
    "    non_empty_indices=[]\n",
    "    \n",
    "    # Iterate over the elements and their indices\n",
    "    for i, sublist in enumerate(list_of_sublists):\n",
    "        if not sublist:  # Check if the sublist is empty\n",
    "            indices.append(i)  # Append the index to the list\n",
    "        else:\n",
    "            non_empty_indices.append(i)\n",
    "            \n",
    "    \n",
    "    return indices,non_empty_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ... (your existing code for indices_of_empty_sublists, data loading, etc.)\n",
    "\n",
    "empty_idx, non_empty_idx = indices_of_empty_sublists(clusters_abs_diff)\n",
    "j = 0\n",
    "\n",
    "# Parameters (same as before)\n",
    "v_max = 25\n",
    "t_min = 0\n",
    "t_max = train_data_2d.shape[-1]\n",
    "f_min = 0\n",
    "f_max = train_data_2d.shape[-2]\n",
    "\n",
    "# Frequency settings (same as before)\n",
    "f_range = (8, 500)\n",
    "desired_ticks = [8, 20, 30, 50, 100, 200, 500]\n",
    "log_base = 10  # Or np.e for natural log\n",
    "\n",
    "def set_frequency_ticks(ax, f_range, desired_ticks, log_base, new_height):\n",
    "    \"\"\"Sets the y-axis (frequency) ticks and labels.\"\"\"\n",
    "    log_f_range = (np.log(f_range[0]) / np.log(log_base), np.log(f_range[1]) / np.log(log_base))\n",
    "    log_desired_ticks = np.log(desired_ticks) / np.log(log_base)\n",
    "\n",
    "    y_ticks_pixel = np.interp(log_desired_ticks, log_f_range, [new_height - 1, 0])\n",
    "\n",
    "    y_ticks_pixel = [int(p) for p in y_ticks_pixel]\n",
    "    y_ticks_pixel = np.clip(y_ticks_pixel, 0, new_height - 1)\n",
    "\n",
    "    y_ticks_pixel, unique_indices = np.unique(y_ticks_pixel, return_index=True)\n",
    "    desired_ticks_used = np.array(desired_ticks)[unique_indices].tolist()\n",
    "\n",
    "    ax.grid(True, axis='y', which='both')\n",
    "    ax.set_yticks(y_ticks_pixel)\n",
    "    ax.set_yticklabels(np.flipud(desired_ticks_used),fontsize=16)\n",
    "    ax.invert_yaxis() # Important: Invert y-axis for spectrograms\n",
    "\n",
    "\n",
    "for i in empty_idx:\n",
    "    if j == 30:\n",
    "        break\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Define fig and axes HERE\n",
    "\n",
    "    # ... (your existing plotting code using axes[0], axes[1], axes[2])\n",
    "\n",
    "    # Plotting\n",
    "    im0 = axes[0].imshow((target_tensor * norm_factor)[i].squeeze(0), cmap='viridis', vmin=0, vmax=v_max, aspect='auto')\n",
    "    axes[0].set_title('Target',fontsize=22)\n",
    "    im1 = axes[1].imshow((generated_tensor * norm_factor)[i].squeeze(0), cmap='viridis', vmin=0, vmax=v_max, aspect='auto')\n",
    "    axes[1].set_title('Generated',fontsize=22)\n",
    "    im2 = axes[2].imshow((abs_difference_tensor)[i].squeeze(0), cmap='viridis', vmin=0, vmax=v_max, aspect='auto')  # Store the image for colorbar\n",
    "    axes[2].set_title('Cleaned',fontsize=22)\n",
    "\n",
    "    for ax in axes: # Apply frequency ticks to all subplots\n",
    "        set_frequency_ticks(ax, f_range, desired_ticks, log_base, target_tensor.shape[-2]) # Use target_tensor or generated_tensor shape\n",
    "        ax.set_xticks([0, 31, 63])\n",
    "        ax.set_xticklabels([0, 0.5, 1],fontsize=16)\n",
    "        ax.set_xlabel(\"Time (s)\",fontsize=20) # Add X label\n",
    "        ax.set_ylabel(\"Frequency (Hz)\",fontsize=20) # Add X label\n",
    "    \n",
    "    fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.03)  # Adjust fraction and pad as needed\n",
    "    \n",
    "    plt.tight_layout() # Adjust subplot params for a tight layout\n",
    "    plt.show()\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_iou_2d_non0(generated_tensor, target_tensor, threshold=20):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) in the 2D plane at the specified intensity threshold for each element in the batch.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_tensor: Tensor containing generated spectrograms (batch_size x 1 x height x width)\n",
    "    - target_tensor: Tensor containing target spectrograms (batch_size x 1 x height x width)\n",
    "    - threshold: Intensity threshold for determining the binary masks\n",
    "\n",
    "    Returns:\n",
    "    - mean_iou: Mean Intersection over Union (IoU) across all elements in the batch\n",
    "    - zero_union_count: Count of elements in the batch with a union of 0\n",
    "    \"\"\"\n",
    "    # Convert intensity threshold to tensor\n",
    "    threshold_tensor = torch.tensor(threshold, device=generated_tensor.device)\n",
    "\n",
    "    # Create binary masks based on the intensity threshold\n",
    "    gen_mask = generated_tensor >= threshold_tensor\n",
    "    tgt_mask = target_tensor >= threshold_tensor\n",
    "\n",
    "    # Convert masks to float tensors\n",
    "    gen_mask = gen_mask.float()\n",
    "    tgt_mask = tgt_mask.float()\n",
    "    print(f'{gen_mask.shape=}')    \n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate intersection and union for each element in the batch\n",
    "    intersection = torch.sum(gen_mask * tgt_mask, dim=(1, 2, 3))\n",
    "    union = torch.sum(gen_mask, dim=(1, 2, 3)) + torch.sum(tgt_mask, dim=(1, 2, 3)) - intersection\n",
    "    print(f'{(intersection/union)[:100]=}')\n",
    "    print(f'{(intersection)[:100]=}')\n",
    "    print(f'{(union)[:100]=}')\n",
    "\n",
    "\n",
    "    # Find elements with union 0\n",
    "    zero_union_mask = union == 0\n",
    "    print(f'{zero_union_mask=}')\n",
    "    zero_union_count = torch.sum(zero_union_mask).item()\n",
    "\n",
    "    # Exclude elements with union 0 from the IoU calculation\n",
    "    iou = intersection / union\n",
    "    iou[zero_union_mask] = 0\n",
    "\n",
    "    # Take mean over non-zero elements in the batch\n",
    "    non_zero_count = len(union) - zero_union_count\n",
    "    mean_iou = torch.sum(iou) / non_zero_count if non_zero_count > 0 else 0\n",
    "    \n",
    "    # Count elements with IoU above 0.9\n",
    "    above_09_count = torch.sum(iou > 0.9).item()\n",
    "\n",
    "    return mean_iou.item()#, zero_union_count, above_09_count\n",
    "    #return mean_iou.item(), zero_union_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_iou_2d_non0(generated_tensor*norm_factor, target_tensor*norm_factor, 15)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4317656,
     "sourceId": 7421593,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pytorch-intertwin",
   "language": "python",
   "name": "pytorch-intertwin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
